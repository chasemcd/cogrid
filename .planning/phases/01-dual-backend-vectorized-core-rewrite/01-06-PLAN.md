---
phase: 01-dual-backend-vectorized-core-rewrite
plan: 06
type: execute
wave: 4
depends_on: ["01-01", "01-02"]
files_modified:
  - cogrid/core/array_rewards.py
autonomous: true

must_haves:
  truths:
    - "Reward functions have signature calculate_reward(prev_state, state, actions) -> reward_array of shape (n_agents,)"
    - "Reward composition resolved at init time, composed function operates on state arrays"
    - "SoupDeliveryReward array version matches existing reward for same state transitions"
    - "OnionInPotReward array version matches existing reward for same state transitions"
    - "Reward values are numerically identical to existing implementation"
  artifacts:
    - path: "cogrid/core/array_rewards.py"
      provides: "Array-based reward functions: delivery_reward_array, onion_in_pot_reward_array, compose_rewards"
      contains: "def delivery_reward_array"
      min_lines: 50
  key_links:
    - from: "cogrid/core/array_rewards.py"
      to: "cogrid/backend"
      via: "xp for array operations"
      pattern: "from cogrid.backend import xp"
    - from: "cogrid/core/array_rewards.py"
      to: "cogrid/core/grid_object.py"
      via: "object_to_idx for type ID lookups"
      pattern: "object_to_idx"
---

<objective>
Create array-based reward functions with the signature `calculate_reward(prev_state, state, actions) -> reward_array`, replacing the Grid-object-based reward system with pure functions on state arrays.

Purpose: The existing reward system calls `isinstance()` on Grid objects and iterates over agents with Python loops. The array-based version uses type ID comparisons on state arrays, producing a `(n_agents,)` reward array. Reward composition (which rewards to include, with what coefficients) is resolved at init time.

Output: New file `cogrid/core/array_rewards.py` with array-based reward functions and composition utility.
</objective>

<execution_context>
@/Users/chasemcd/.claude/get-shit-done/workflows/execute-plan.md
@/Users/chasemcd/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/01-dual-backend-vectorized-core-rewrite/01-RESEARCH.md
@.planning/phases/01-dual-backend-vectorized-core-rewrite/01-CONTEXT.md
@.planning/phases/01-dual-backend-vectorized-core-rewrite/01-01-SUMMARY.md
@.planning/phases/01-dual-backend-vectorized-core-rewrite/01-02-SUMMARY.md
@cogrid/envs/overcooked/rewards.py
@cogrid/core/reward.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create array-based reward functions</name>
  <files>cogrid/core/array_rewards.py</files>
  <action>
Create NEW file `cogrid/core/array_rewards.py` with pure-function reward implementations.

**Common signature for all reward functions:**

```python
def reward_fn(
    prev_state: dict,   # Dict of previous state arrays
    state: dict,         # Dict of current state arrays
    actions: ndarray,    # (n_agents,) int32
    agent_ids: list,     # List of agent IDs (for indexing)
    type_ids: dict,      # Maps type name -> type_id
    coefficient: float,  # Reward scaling
    common_reward: bool, # Whether reward is shared across agents
) -> ndarray:            # (n_agents,) float32
```

Where `state` dict contains: `agent_pos`, `agent_dir`, `agent_inv`, `object_type_map`, `object_state_map`, `pot_contents`, `pot_timer`, `pot_positions`.

**1. `delivery_reward_array()` -- matches SoupDeliveryReward:**

Existing logic (rewards.py:28-77):
- For each agent taking PickupDrop action:
  - Check if agent holds OnionSoup (or TomatoSoup -- check existing code, it only checks OnionSoup. Wait, re-read: `isinstance(obj, overcooked_grid_objects.OnionSoup)`. So it ONLY rewards OnionSoup delivery. TomatoSoup delivery is not rewarded. Preserve this.)
  - Check if agent faces a DeliveryZone
  - If both: add coefficient to reward

Array version:
```python
def delivery_reward_array(prev_state, state, actions, type_ids, n_agents, coefficient=1.0, common_reward=True, action_pickup_drop_idx=4):
    from cogrid.backend import xp

    rewards = xp.zeros(n_agents, dtype=xp.float32)
    dir_vec_table = ...  # get from agent module

    for i in range(n_agents):
        if actions[i] != action_pickup_drop_idx:
            continue

        # Check if agent holds OnionSoup (check prev_state inventory, since this is evaluated before the interaction modifies it)
        agent_holds_soup = prev_state['agent_inv'][i, 0] == type_ids['onion_soup']

        # Check if agent faces delivery zone
        fwd_pos = prev_state['agent_pos'][i] + dir_vec_table[prev_state['agent_dir'][i]]
        fwd_type = prev_state['object_type_map'][fwd_pos[0], fwd_pos[1]]
        facing_delivery = fwd_type == type_ids['delivery_zone']

        if agent_holds_soup and facing_delivery:
            if common_reward:
                rewards = rewards + coefficient  # all agents get it
            else:
                rewards[i] += coefficient
            break  # only one delivery per step in common mode

    return rewards
```

WAIT -- re-check existing code flow. Rewards are computed in `compute_rewards()` (line 700-726) AFTER `interact()`. The reward's `calculate_reward()` receives `state=self.prev_grid` (the grid BEFORE the step). So rewards look at the PREVIOUS grid state, PREVIOUS agent actions, and the NEW grid state. But `SoupDeliveryReward.calculate_reward()` checks `state.grid_agents[agent_id]` (from `prev_grid`) for inventory, and `state.get(*fwd_pos)` (from `prev_grid`) for the delivery zone. So it uses the PRE-step state for both inventory and grid.

Actually, re-reading more carefully: `state` parameter IS `self.prev_grid` and `new_state` IS `self.grid`. The reward checks the PREVIOUS state's agent inventory and the PREVIOUS state's grid. So the reward is: "at the beginning of this step, was the agent holding soup and facing a delivery zone, and did they try to drop it?" The actual drop happens in `interact()`, but the reward evaluates the CONDITIONS for the drop, not whether it succeeded.

For the array version: use `prev_state` dict (the state before interactions happened). Check prev_state's agent_inv and prev_state's object_type_map.

Mark all loops with `# PHASE2: vectorize across agents`.

**2. `onion_in_pot_reward_array()` -- matches OnionInPotReward:**

Existing logic (rewards.py:100-147):
- For each agent taking PickupDrop:
  - Check agent holds Onion (from prev_state)
  - Check agent faces Pot with capacity (from prev_state)
  - If both: reward

Array version:
- Check `prev_state['agent_inv'][i, 0] == type_ids['onion']`
- Check fwd cell is pot: `fwd_type == type_ids['pot']`
- Check pot has capacity: find pot index, check `sum(pot_contents[pot_idx] != -1) < 3`
- Check ingredient type matches (same as place_on logic)

**3. `soup_in_dish_reward_array()` -- matches SoupInDishReward:**

Existing logic (rewards.py:163-208):
- For each agent taking PickupDrop:
  - Check agent holds Plate
  - Check agent faces ready Pot (timer == 0)
  - If both: reward

Array version:
- Check `prev_state['agent_inv'][i, 0] == type_ids['plate']`
- Check fwd cell is pot
- Check `pot_timer[pot_idx] == 0`

**4. `compose_rewards()` utility:**

```python
def compose_rewards(reward_configs: list[dict]) -> callable:
    """Build a composed reward function from reward configs.

    Called at init time. Each config has: name, fn, coefficient, common_reward.
    Returns a function (prev_state, state, actions) -> reward_array that sums all rewards.
    """
    def composed_reward(prev_state, state, actions, n_agents, type_ids, action_pickup_drop_idx):
        from cogrid.backend import xp
        total = xp.zeros(n_agents, dtype=xp.float32)
        for config in reward_configs:
            r = config['fn'](prev_state, state, actions, type_ids, n_agents,
                           coefficient=config['coefficient'],
                           common_reward=config.get('common_reward', False),
                           action_pickup_drop_idx=action_pickup_drop_idx)
            total = total + r
        return total
    return composed_reward
```

The composed function captures the reward configs via closure.

NOTE: These functions do NOT replace the existing Reward classes. They exist alongside them. The integration plan (Plan 07) wires them into the step loop.

Add `test_reward_parity()` function that compares array rewards against existing Reward classes for the same state transitions.
  </action>
  <verify>
Run: `python -c "
import numpy as np
import cogrid.envs
from cogrid.core.array_rewards import delivery_reward_array, onion_in_pot_reward_array, soup_in_dish_reward_array
from cogrid.core.grid_object import object_to_idx, get_object_names
from cogrid.core.agent import get_dir_vec_table

scope = 'overcooked'
type_ids = {
    'onion_soup': object_to_idx('onion_soup', scope=scope),
    'tomato_soup': object_to_idx('tomato_soup', scope=scope),
    'onion': object_to_idx('onion', scope=scope),
    'plate': object_to_idx('plate', scope=scope),
    'pot': object_to_idx('pot', scope=scope),
    'delivery_zone': object_to_idx('delivery_zone', scope=scope),
}

# Test delivery reward: agent 0 holds onion_soup, faces delivery zone
prev_state = {
    'agent_pos': np.array([[1, 1], [3, 3]], dtype=np.int32),
    'agent_dir': np.array([0, 0], dtype=np.int32),  # facing Right
    'agent_inv': np.array([[type_ids['onion_soup']], [-1]], dtype=np.int32),
    'object_type_map': np.zeros((5, 5), dtype=np.int32),
    'object_state_map': np.zeros((5, 5), dtype=np.int32),
    'pot_contents': np.array([[-1, -1, -1]], dtype=np.int32),
    'pot_timer': np.array([30], dtype=np.int32),
    'pot_positions': [(0, 0)],
}
prev_state['object_type_map'][1, 2] = type_ids['delivery_zone']  # delivery zone in front

actions = np.array([4, 6], dtype=np.int32)  # agent 0 = PickupDrop, agent 1 = Noop
rewards = delivery_reward_array(prev_state, prev_state, actions, type_ids, 2, coefficient=1.0, common_reward=True, action_pickup_drop_idx=4)
print('Delivery rewards:', rewards)
assert rewards[0] == 1.0 and rewards[1] == 1.0, f'Common reward should give 1.0 to both: {rewards}'
print('delivery_reward_array: PASSED')

# Test onion_in_pot: agent holds onion, faces pot with capacity
prev_state2 = {
    'agent_pos': np.array([[1, 1]], dtype=np.int32),
    'agent_dir': np.array([0], dtype=np.int32),
    'agent_inv': np.array([[type_ids['onion']]], dtype=np.int32),
    'object_type_map': np.zeros((5, 5), dtype=np.int32),
    'object_state_map': np.zeros((5, 5), dtype=np.int32),
    'pot_contents': np.array([[-1, -1, -1]], dtype=np.int32),
    'pot_timer': np.array([30], dtype=np.int32),
    'pot_positions': [(1, 2)],
}
prev_state2['object_type_map'][1, 2] = type_ids['pot']

actions2 = np.array([4], dtype=np.int32)
rewards2 = onion_in_pot_reward_array(prev_state2, prev_state2, actions2, type_ids, 1, coefficient=0.1, action_pickup_drop_idx=4)
print('OnionInPot rewards:', rewards2)
assert rewards2[0] == 0.1, f'Should be 0.1: {rewards2[0]}'
print('onion_in_pot_reward_array: PASSED')

print('ALL REWARD TESTS PASSED')
"` succeeds.

Run existing tests: `python -m pytest cogrid/test_gridworld_env.py cogrid/test_overcooked_env.py -x -q` -- all pass.
  </verify>
  <done>
Array-based reward functions exist: delivery_reward_array, onion_in_pot_reward_array, soup_in_dish_reward_array. All have signature `(prev_state, state, actions, ...) -> (n_agents,)`. Composition utility resolves at init time. Reward parity test confirms identical values to existing Reward classes. No regression.
  </done>
</task>

</tasks>

<verification>
1. All reward functions produce correct values for test scenarios
2. Reward composition sums multiple rewards correctly
3. Parity test confirms identical values to existing implementation
4. Rewards use type_ids instead of isinstance()
5. All existing tests pass unchanged
</verification>

<success_criteria>
- Array-based reward functions with `(prev_state, state, actions) -> reward_array` signature
- Reward composition resolved at init time
- Reward parity test passes for deterministic scenarios
- No isinstance() in reward step path
- No regression in existing tests
</success_criteria>

<output>
After completion, create `.planning/phases/01-dual-backend-vectorized-core-rewrite/01-06-SUMMARY.md`
</output>
