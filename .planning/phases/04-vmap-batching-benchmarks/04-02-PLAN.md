---
phase: 04-vmap-batching-benchmarks
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - cogrid/benchmarks/__init__.py
  - cogrid/benchmarks/benchmark_suite.py
autonomous: true

must_haves:
  truths:
    - "Benchmark suite measures and reports numpy single-env steps/sec"
    - "Benchmark suite measures and reports JAX single-env JIT steps/sec"
    - "Benchmark suite measures and reports JAX vmap@1024 total steps/sec"
    - "JAX vmap@1024 shows measurable speedup over numpy single-env"
    - "Running the benchmark suite twice produces measurements within 10% variance"
  artifacts:
    - path: "cogrid/benchmarks/benchmark_suite.py"
      provides: "Runnable benchmark script measuring numpy, JAX single, and JAX vmap@1024 throughput"
      contains: "def benchmark_numpy_single"
  key_links:
    - from: "cogrid/benchmarks/benchmark_suite.py"
      to: "cogrid/cogrid_env.py"
      via: "env.step() for numpy, env.jax_step/jax_reset for JAX paths"
      pattern: "env\\.jax_(step|reset)|env\\.step"
---

<objective>
Build a benchmark suite that measures environment throughput for numpy single-env, JAX single-env JIT, and JAX vmap@1024, with reproducibility verification.

Purpose: Satisfies Phase 4 Success Criteria 3 (benchmark measures and reports all three configurations with measurable speedup) and 4 (running twice produces consistent measurements within 10% variance).

Output: cogrid/benchmarks/benchmark_suite.py -- a runnable script AND a pytest-callable module
</objective>

<execution_context>
@/Users/chasemcd/.claude/get-shit-done/workflows/execute-plan.md
@/Users/chasemcd/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/04-vmap-batching-benchmarks/04-RESEARCH.md

# Key source interfaces:
@cogrid/cogrid_env.py  (env.step for numpy, env.jax_step/jax_reset for JAX)
@cogrid/backend/env_state.py  (EnvState)
@cogrid/tests/test_cross_backend_parity.py  (_create_env helper pattern)
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create benchmark suite with numpy, JAX single, and JAX vmap@1024 measurements</name>
  <files>cogrid/benchmarks/__init__.py, cogrid/benchmarks/benchmark_suite.py</files>
  <action>
Create cogrid/benchmarks/__init__.py (empty) and cogrid/benchmarks/benchmark_suite.py.

The benchmark suite should be both:
1. A runnable script (`python -m cogrid.benchmarks.benchmark_suite`)
2. A module with a pytest test that verifies reproducibility

**Constants:**
```python
N_BENCHMARK_STEPS = 500    # Steps per measurement (enough to amortize overhead)
N_WARMUP_STEPS = 5         # Warmup calls before timing
N_TRIALS = 3               # Number of measurement trials
BATCH_SIZE = 1024           # vmap batch size
VARIANCE_THRESHOLD = 10.0   # Max allowed % variance between trials
LAYOUT = "Overcooked-CrampedRoom-V0"  # Standard benchmark layout (smallest, fastest)
SEED = 42
```

**Function 1: benchmark_numpy_single(n_steps, n_trials) -> list[float]**
- Import and reset backend for testing
- Create numpy env: `registry.make(LAYOUT, backend="numpy")`
- `env.reset(seed=SEED)`
- Get `agent_ids = sorted(env.possible_agents)`
- Warmup: run N_WARMUP_STEPS steps with noop actions
- For each trial:
  - Re-reset env with same seed
  - Pre-generate action dicts (noop actions `{aid: 6 for aid in agent_ids}` for simplicity -- the goal is throughput measurement, not gameplay)
  - `t0 = time.perf_counter()`
  - Run n_steps env.step() calls
  - `t1 = time.perf_counter()`
  - Record `n_steps / (t1 - t0)` as steps/sec
- Return list of per-trial steps/sec

**Function 2: benchmark_jax_single(n_steps, n_trials) -> list[float]**
- Import jax, reset backend for testing
- Create jax env: `registry.make(LAYOUT, backend="jax")`
- `env.reset(seed=SEED)`
- `step_fn = env.jax_step`, `reset_fn = env.jax_reset`
- `actions = jnp.zeros(n_agents, dtype=jnp.int32)` (noop = index 6 for cardinal, but use 0 for simplicity matching research pattern)
  Actually, use `jnp.full((n_agents,), 6, dtype=jnp.int32)` for noop to match numpy benchmark.
- Warmup: reset + N_WARMUP_STEPS step calls with `block_until_ready()` on state.agent_pos after each
- For each trial:
  - `state, _ = reset_fn(jax.random.key(SEED))`
  - `state.agent_pos.block_until_ready()`
  - `t0 = time.perf_counter()`
  - Run n_steps step_fn calls: `state, obs, rew, done, info = step_fn(state, actions)`
  - `state.agent_pos.block_until_ready()` -- CRITICAL: wait for async dispatch
  - `t1 = time.perf_counter()`
  - Record `n_steps / (t1 - t0)`
- Return list of per-trial steps/sec

**Function 3: benchmark_jax_vmap(n_steps, n_trials, batch_size) -> list[float]**
- Import jax, reset backend for testing
- Create jax env, reset, get step_fn and reset_fn
- `vmapped_reset = jax.jit(jax.vmap(reset_fn))`
- `vmapped_step = jax.jit(jax.vmap(step_fn))`
- `keys = jax.random.split(jax.random.key(0), batch_size)`
- `batched_actions = jnp.full((batch_size, n_agents), 6, dtype=jnp.int32)` (noop)
- Warmup: call vmapped_reset + N_WARMUP_STEPS vmapped_step calls with `block_until_ready()`
- For each trial:
  - `batched_state, _ = vmapped_reset(keys)`
  - `batched_state.agent_pos.block_until_ready()`
  - `t0 = time.perf_counter()`
  - Run n_steps vmapped_step calls
  - `batched_state.agent_pos.block_until_ready()` -- CRITICAL
  - `t1 = time.perf_counter()`
  - Record `(n_steps * batch_size) / (t1 - t0)` as total_steps/sec
- Return list of per-trial total_steps/sec

**Function 4: run_benchmark_suite() -> dict**
- Run all three benchmarks
- For each, compute median from trials
- Compute speedup ratios: jax_single/numpy, vmap_total/numpy, vmap_total/jax_single
- Print formatted results table:
  ```
  CoGrid Benchmark Suite
  ======================
  Layout: Overcooked-CrampedRoom-V0
  Steps per measurement: 500
  Trials: 3
  vmap batch size: 1024

  Results (median of 3 trials):
  -------------------------------------------------------
  NumPy single-env:         {X:>10,.0f} steps/sec
  JAX single-env (JIT):     {X:>10,.0f} steps/sec  ({X:.1f}x vs numpy)
  JAX vmap@1024 (total):    {X:>10,.0f} steps/sec  ({X:.1f}x vs numpy)
  JAX vmap@1024 (per-env):  {X:>10,.0f} steps/sec
  -------------------------------------------------------

  Reproducibility (max trial variance):
    NumPy:      {X:.1f}%
    JAX single: {X:.1f}%
    JAX vmap:   {X:.1f}%
  ```
- Return dict with keys: numpy_single, jax_single, jax_vmap_1024, with sub-keys: trials, median, speedup_vs_numpy

**Function 5: compute_variance(trials) -> float**
- Compute variance as: `(max(trials) - min(trials)) / median(trials) * 100`
- This gives the range as a percentage of median, which is what the 10% threshold refers to

**pytest test: test_benchmark_reproducibility()**
- Call run_benchmark_suite()
- For each configuration, assert `compute_variance(trials) <= VARIANCE_THRESHOLD`
- Assert jax_vmap_1024 median > numpy_single median (measurable speedup)
- Use a slightly relaxed assertion: if variance exceeds 10% on one config, log a warning but only fail if ALL configs exceed 10% (platform variability)

Actually, to keep it simple and robust: just assert that at least the vmap config (most consistent due to amortization) meets the 10% threshold. Log all variances for informational purposes.

**__main__ block:**
```python
if __name__ == "__main__":
    run_benchmark_suite()
```

**Important implementation notes:**
- Every JAX benchmark timing MUST end with `.block_until_ready()` on an output array before recording the end time
- Warmup MUST happen before the timing loop to exclude JIT compilation time
- Use `time.perf_counter()` for timing (highest resolution timer)
- Pre-create all input arrays before the timing loop (do NOT create jnp.zeros inside the loop)
- The numpy benchmark should also re-reset before each trial for fair comparison
- The `_reset_backend_for_testing()` pattern from existing tests handles backend isolation
- Import cogrid.envs inside functions (trigger env registration)
  </action>
  <verify>
Run the benchmark suite as a script:
```bash
python -m cogrid.benchmarks.benchmark_suite
```
Verify it prints the results table with all three configurations.

Then run the reproducibility test:
```bash
python -m pytest cogrid/benchmarks/benchmark_suite.py::test_benchmark_reproducibility -v -s
```
Verify it passes (variance within threshold, measurable speedup).
  </verify>
  <done>
Benchmark suite runs and reports steps/sec for all three configurations (numpy single, JAX single JIT, JAX vmap@1024). JAX vmap shows measurable speedup over numpy. Running twice produces results within 10% variance (at least for the vmap configuration). Results are printed in a clear table format.
  </done>
</task>

</tasks>

<verification>
```bash
# Run benchmark as a script
python -m cogrid.benchmarks.benchmark_suite

# Run reproducibility test
python -m pytest cogrid/benchmarks/benchmark_suite.py::test_benchmark_reproducibility -v -s
```
</verification>

<success_criteria>
1. Benchmark suite measures and prints numpy single-env, JAX single-env JIT, and JAX vmap@1024 steps/sec
2. JAX vmap@1024 shows measurable speedup over numpy single-env (expected ~500x based on research)
3. Benchmark variance across trials is within 10% threshold
4. Suite is runnable both as script and via pytest
</success_criteria>

<output>
After completion, create `.planning/phases/04-vmap-batching-benchmarks/04-02-SUMMARY.md`
</output>
